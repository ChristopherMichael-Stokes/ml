{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"Qwen/Qwen1.5-7B-Chat\"\n",
    "model_dir = \"qwen-2-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = st.SentenceTransformer(model_name_or_path=\"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.save('bge-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}  # set True to compute cosine similarity\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name='bge-large',\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"Represent this sentence for searching relevant passages: \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f958a2188634ec68cb51de2fac9baeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\miniconda3\\envs\\ai\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chris\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-7B-Chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3d261ca28f4c6b8debd585c4ebd8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/31.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1c9934ff5f4b5da89f74ee603ac897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876d6193ad8e433bbe0310ec67d6c04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace88f7094ca4326bfe6a275e762c7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6feb3a60a14fd2b747829c3ff4770a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682954ac40484590b5ae9dceefbabd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f757b7b3ae477aa7a56d5333d100b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91903846f0aa4ecb80509300515579db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933074bd80634f5fb0d1ae4ac25aed05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5d987bfeb04aa0a8a4a06a86d56f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa67dce6b75c4b90a64807c36de8741a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec64437cb3f40d7bfc9041cba76eda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'model.layers.27.self_attn.k_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.8.input_layernorm.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.31.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.12.mlp.down_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.15.input_layernorm.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.27.mlp.down_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.25.input_layernorm.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.input_layernorm.weight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.20.mlp.up_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.21.input_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.9.mlp.down_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.27.input_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.13.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.28.mlp.up_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.20.mlp.down_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.29.input_layernorm.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.30.input_layernorm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.mlp.down_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.input_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.21.mlp.up_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.17.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.input_layernorm.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.25.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.14.input_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.23.mlp.up_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.norm.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.26.mlp.down_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.10.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.18.mlp.up_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.30.post_attention_layernorm.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(model_dir)\n\u001b[0;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(model_dir)\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\ai\\Lib\\site-packages\\transformers\\modeling_utils.py:2469\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   2465\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_file, shard \u001b[38;5;129;01min\u001b[39;00m shards\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[0;32m   2467\u001b[0m         \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m         \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[1;32m-> 2469\u001b[0m         safe_save_file(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m   2470\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2471\u001b[0m         save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\ai\\Lib\\site-packages\\safetensors\\torch.py:281\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[0;32m    251\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    252\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    253\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    254\u001b[0m ):\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\ai\\Lib\\site-packages\\safetensors\\torch.py:485\u001b[0m, in \u001b[0;36m_flatten\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    486\u001b[0m     k: {\n\u001b[0;32m    487\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: _tobytes(v, k),\n\u001b[0;32m    490\u001b[0m     }\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    492\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\ai\\Lib\\site-packages\\safetensors\\torch.py:489\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    486\u001b[0m     k: {\n\u001b[0;32m    487\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: v\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m--> 489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: _tobytes(v, k),\n\u001b[0;32m    490\u001b[0m     }\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    492\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\ai\\Lib\\site-packages\\safetensors\\torch.py:411\u001b[0m, in \u001b[0;36m_tobytes\u001b[1;34m(tensor, name)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` which is not allowed. It either means you\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms recommended to save\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pack it before saving.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m     )\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\miniconda3\\envs\\ai\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"C:\\\\Users\\\\chris\\\\.cache\\\\huggingface\\\\hub\\\\models--Qwen--Qwen1.5-7B-Chat\\\\snapshots\\\\a2662f4bc1afe913a91cd49f794d229a8c28f97e\\\\*\" \"$model_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: qwen-2-7b\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "Set model parameters\n",
      "Set model tokenizer\n",
      "gguf: Adding 151387 merge(s).\n",
      "gguf: Setting special token type eos to 151645\n",
      "gguf: Setting special token type pad to 151643\n",
      "gguf: Setting special token type bos to 151643\n",
      "gguf: Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|im_end|>' + '\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Exporting model to 'qwen-2-7b\\ggml-model-f16.gguf'\n",
      "gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "token_embd.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.0.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.0.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.0.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.0.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.0.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.0.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.0.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.0.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.0.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.0.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.0.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.0.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.1.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.1.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.1.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.1.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.1.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.1.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.1.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.1.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.1.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.1.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.1.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.1.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.2.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.2.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.2.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.2.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.2.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.2.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.2.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.2.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.2.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.2.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.2.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.2.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.3.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.3.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.3.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.3.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.3.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.3.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.3.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.3.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.3.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.3.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.3.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.3.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.4.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.4.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.4.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.4.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.4.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.4.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.4.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.4.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.4.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.4.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.4.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.4.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.5.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.5.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.5.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.5.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.5.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.5.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.5.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.5.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.5.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.5.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.5.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.5.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.6.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.6.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.6.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.6.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.6.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.6.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.6.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.6.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.6.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "blk.10.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.10.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.10.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.10.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.10.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.10.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.10.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.10.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.10.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.10.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.10.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.10.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.11.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.11.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.11.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.11.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.11.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.11.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.11.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.11.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.11.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.11.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.11.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.11.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.12.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.12.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.12.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.12.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.12.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.12.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.12.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.12.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.12.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.12.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.12.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.12.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.13.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.13.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.13.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.13.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.13.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.13.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.13.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.13.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.13.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.13.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.13.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.13.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.14.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.14.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.14.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.14.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.14.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.14.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.14.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.14.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.14.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.14.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.14.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.14.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.15.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.15.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.15.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.15.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.15.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.15.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.15.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.15.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.15.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.15.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.15.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.15.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.16.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.16.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.16.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.16.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.16.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.16.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.16.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.16.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.6.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.6.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.6.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.7.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.7.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.7.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.7.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.7.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.7.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.7.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.7.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.7.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.7.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.7.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.7.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.8.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.8.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.8.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.8.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.8.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.8.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.8.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.8.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.8.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.8.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.8.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.8.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.9.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.9.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.9.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.9.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.9.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.9.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.9.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.9.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.9.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.9.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.9.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.9.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "blk.16.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.16.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.16.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.16.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.17.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.17.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.17.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.17.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.17.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.17.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.17.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.17.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.17.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.17.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.17.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.17.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.18.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.18.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.18.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.18.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.18.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.18.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.18.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.18.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.18.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.18.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.18.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.18.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.19.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.19.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.19.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.19.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.19.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.19.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.19.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.19.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.19.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.19.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.19.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.19.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.20.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.20.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.20.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.20.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.20.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.20.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.20.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.20.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.20.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.20.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.20.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.20.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.21.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.21.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.21.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.21.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.21.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.21.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.21.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.21.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.21.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.21.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.21.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.21.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.22.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.22.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.22.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.22.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.22.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.22.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.22.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.22.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.22.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.22.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.22.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.22.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.23.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.23.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.23.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.23.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.23.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.23.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.23.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.23.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.23.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.23.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.23.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.23.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.24.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.24.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.24.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.24.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.24.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.24.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.24.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.24.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.24.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.24.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.24.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.24.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.25.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.25.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.25.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.25.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.25.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.25.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.25.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.25.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.25.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.25.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.25.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.25.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.26.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.26.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.26.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.26.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.26.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.26.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.26.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.26.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.26.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.26.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.26.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.26.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.27.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.27.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.27.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.27.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.27.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.27.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.27.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.27.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.27.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.27.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.27.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.27.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.28.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.28.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.28.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.28.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.28.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.28.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.28.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.28.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.28.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.28.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.28.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.28.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.29.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.29.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.29.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.29.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.29.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.29.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.29.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.29.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.29.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.29.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.29.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.29.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.30.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.30.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.30.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.30.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.30.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.30.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.30.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.30.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.30.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.30.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.30.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.30.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.31.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.31.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.31.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.31.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.31.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.31.attn_k.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.31.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.31.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.31.attn_q.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.31.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "blk.31.attn_v.bias, n_dims = 1, torch.bfloat16 --> float32\n",
      "blk.31.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16\n",
      "output_norm.weight, n_dims = 1, torch.bfloat16 --> float32\n",
      "Model successfully exported to 'qwen-2-7b\\ggml-model-f16.gguf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python ../llama.cpp/convert-hf-to-gguf.py \"$model_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time = 48634.46 ms\n",
      "main:    total time = 48634.46 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 2329 (67be2ce1)\n",
      "main: built with cc (GCC) 13.2.0 for x86_64-w64-mingw32\n",
      "main: quantizing '../notebooks/qwen-2-7b/ggml-model-f16.gguf' to '../notebooks/qwen-2-7b/ggml-model-Q4_K_M.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 387 tensors from ../notebooks/qwen-2-7b/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = qwen-2-7b\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llama_model_quantize_internal: meta size = 5950880 bytes\n",
      "[   1/ 387]                    token_embd.weight - [ 4096, 151936,     1,     1], type =    f16, quantizing to q4_K .. size =  1187.00 MiB ->   333.84 MiB\n",
      "[   2/ 387]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 387]                blk.0.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[   4/ 387]                blk.0.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[   5/ 387]                  blk.0.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[   6/ 387]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 387]                    blk.0.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   8/ 387]                  blk.0.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[   9/ 387]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  10/ 387]                    blk.0.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  11/ 387]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  12/ 387]                    blk.0.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  13/ 387]                  blk.0.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  14/ 387]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  15/ 387]                blk.1.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  16/ 387]                blk.1.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  17/ 387]                  blk.1.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  18/ 387]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  19/ 387]                    blk.1.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  20/ 387]                  blk.1.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  21/ 387]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  22/ 387]                    blk.1.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  23/ 387]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  24/ 387]                    blk.1.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  25/ 387]                  blk.1.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  26/ 387]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  27/ 387]                blk.2.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  28/ 387]                blk.2.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  29/ 387]                  blk.2.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  30/ 387]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  31/ 387]                    blk.2.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  32/ 387]                  blk.2.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  33/ 387]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  34/ 387]                    blk.2.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  35/ 387]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  36/ 387]                    blk.2.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  37/ 387]                  blk.2.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  38/ 387]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 387]                blk.3.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  40/ 387]                blk.3.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  41/ 387]                  blk.3.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  42/ 387]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  43/ 387]                    blk.3.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  44/ 387]                  blk.3.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  45/ 387]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  46/ 387]                    blk.3.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  47/ 387]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  48/ 387]                    blk.3.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  49/ 387]                  blk.3.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  50/ 387]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  51/ 387]                blk.4.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  52/ 387]                blk.4.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  53/ 387]                  blk.4.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  54/ 387]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  55/ 387]                    blk.4.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  56/ 387]                  blk.4.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  57/ 387]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  58/ 387]                    blk.4.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  59/ 387]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  60/ 387]                    blk.4.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  61/ 387]                  blk.4.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  62/ 387]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  63/ 387]                blk.5.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  64/ 387]                blk.5.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  65/ 387]                  blk.5.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  66/ 387]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  67/ 387]                    blk.5.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  68/ 387]                  blk.5.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  69/ 387]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  70/ 387]                    blk.5.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  71/ 387]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  72/ 387]                    blk.5.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  73/ 387]                  blk.5.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  74/ 387]                blk.6.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  75/ 387]                  blk.6.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  76/ 387]                    blk.6.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  77/ 387]                  blk.6.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  78/ 387]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  79/ 387]                    blk.6.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  80/ 387]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  81/ 387]                    blk.6.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  82/ 387]                  blk.6.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[  83/ 387]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 387]               blk.10.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[  85/ 387]               blk.10.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  86/ 387]                 blk.10.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  87/ 387]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  88/ 387]                   blk.10.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  89/ 387]                 blk.10.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  90/ 387]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  91/ 387]                   blk.10.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  92/ 387]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  93/ 387]                   blk.10.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 387]                 blk.10.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[  95/ 387]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  96/ 387]               blk.11.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  97/ 387]               blk.11.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  98/ 387]                 blk.11.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[  99/ 387]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 100/ 387]                   blk.11.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 101/ 387]                 blk.11.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 102/ 387]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 103/ 387]                   blk.11.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 104/ 387]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 105/ 387]                   blk.11.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 387]                 blk.11.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 107/ 387]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 108/ 387]               blk.12.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 109/ 387]               blk.12.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 110/ 387]                 blk.12.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 111/ 387]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 112/ 387]                   blk.12.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 113/ 387]                 blk.12.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 114/ 387]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 115/ 387]                   blk.12.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 116/ 387]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 117/ 387]                   blk.12.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 118/ 387]                 blk.12.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 119/ 387]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 387]               blk.13.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 121/ 387]               blk.13.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 122/ 387]                 blk.13.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 123/ 387]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 387]                   blk.13.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 125/ 387]                 blk.13.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 126/ 387]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 127/ 387]                   blk.13.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 128/ 387]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 129/ 387]                   blk.13.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 130/ 387]                 blk.13.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 131/ 387]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 132/ 387]               blk.14.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 133/ 387]               blk.14.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 134/ 387]                 blk.14.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 135/ 387]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 136/ 387]                   blk.14.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 137/ 387]                 blk.14.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 138/ 387]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 139/ 387]                   blk.14.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 140/ 387]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 141/ 387]                   blk.14.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 387]                 blk.14.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 143/ 387]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 144/ 387]               blk.15.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 145/ 387]               blk.15.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 146/ 387]                 blk.15.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 147/ 387]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 148/ 387]                   blk.15.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 149/ 387]                 blk.15.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 150/ 387]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 151/ 387]                   blk.15.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 152/ 387]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 153/ 387]                   blk.15.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 154/ 387]                 blk.15.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 155/ 387]               blk.16.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 156/ 387]                   blk.16.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 157/ 387]                 blk.16.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 158/ 387]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 159/ 387]                   blk.16.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 387]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 161/ 387]                   blk.16.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 162/ 387]                 blk.16.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 163/ 387]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 164/ 387]                blk.6.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 165/ 387]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 166/ 387]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 167/ 387]                blk.7.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 168/ 387]                blk.7.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 169/ 387]                  blk.7.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 170/ 387]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 171/ 387]                    blk.7.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 172/ 387]                  blk.7.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 173/ 387]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 174/ 387]                    blk.7.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 175/ 387]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 176/ 387]                    blk.7.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 177/ 387]                  blk.7.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 178/ 387]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 179/ 387]                blk.8.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 180/ 387]                blk.8.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 181/ 387]                  blk.8.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 182/ 387]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 387]                    blk.8.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 184/ 387]                  blk.8.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 185/ 387]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 186/ 387]                    blk.8.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 387]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 188/ 387]                    blk.8.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 189/ 387]                  blk.8.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 190/ 387]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 191/ 387]                blk.9.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 192/ 387]                blk.9.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 193/ 387]                  blk.9.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 194/ 387]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 195/ 387]                    blk.9.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 387]                  blk.9.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 197/ 387]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 198/ 387]                    blk.9.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 199/ 387]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 200/ 387]                    blk.9.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 387]                  blk.9.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 202/ 387]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 203/ 387]               blk.16.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 204/ 387]                 blk.16.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 205/ 387]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 206/ 387]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 207/ 387]               blk.17.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 208/ 387]               blk.17.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 209/ 387]                 blk.17.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 210/ 387]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 211/ 387]                   blk.17.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 212/ 387]                 blk.17.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 213/ 387]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 214/ 387]                   blk.17.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 215/ 387]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 216/ 387]                   blk.17.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 217/ 387]                 blk.17.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 218/ 387]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 387]               blk.18.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 220/ 387]               blk.18.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 221/ 387]                 blk.18.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 222/ 387]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 223/ 387]                   blk.18.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 224/ 387]                 blk.18.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 225/ 387]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 226/ 387]                   blk.18.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 227/ 387]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 228/ 387]                   blk.18.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 229/ 387]                 blk.18.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 230/ 387]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 231/ 387]               blk.19.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 232/ 387]               blk.19.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 233/ 387]                 blk.19.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 234/ 387]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 235/ 387]                   blk.19.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 236/ 387]                 blk.19.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 237/ 387]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 238/ 387]                   blk.19.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 239/ 387]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 240/ 387]                   blk.19.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 241/ 387]                 blk.19.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 242/ 387]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 243/ 387]               blk.20.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 244/ 387]               blk.20.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 245/ 387]                 blk.20.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 246/ 387]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 387]                   blk.20.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 248/ 387]                 blk.20.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 249/ 387]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 250/ 387]                   blk.20.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 251/ 387]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 252/ 387]                   blk.20.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 253/ 387]                 blk.20.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 254/ 387]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 387]               blk.21.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 256/ 387]               blk.21.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 257/ 387]                 blk.21.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 258/ 387]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 259/ 387]                   blk.21.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 260/ 387]                 blk.21.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 261/ 387]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 262/ 387]                   blk.21.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 263/ 387]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 264/ 387]                   blk.21.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 265/ 387]                 blk.21.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 266/ 387]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 267/ 387]               blk.22.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 268/ 387]               blk.22.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 269/ 387]                 blk.22.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 270/ 387]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 271/ 387]                   blk.22.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 272/ 387]                 blk.22.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 273/ 387]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 274/ 387]                   blk.22.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 275/ 387]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 276/ 387]                   blk.22.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 277/ 387]                 blk.22.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 278/ 387]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 279/ 387]               blk.23.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 280/ 387]               blk.23.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 281/ 387]                 blk.23.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 282/ 387]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 387]                   blk.23.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 284/ 387]                 blk.23.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 285/ 387]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 286/ 387]                   blk.23.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 287/ 387]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 288/ 387]                   blk.23.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 289/ 387]                 blk.23.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 290/ 387]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 387]               blk.24.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 292/ 387]               blk.24.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 293/ 387]                 blk.24.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 294/ 387]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 295/ 387]                   blk.24.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 296/ 387]                 blk.24.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 297/ 387]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 298/ 387]                   blk.24.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 299/ 387]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 300/ 387]                   blk.24.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 301/ 387]                 blk.24.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 302/ 387]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 303/ 387]               blk.25.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 304/ 387]               blk.25.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 305/ 387]                 blk.25.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 306/ 387]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 307/ 387]                   blk.25.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 308/ 387]                 blk.25.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 309/ 387]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 310/ 387]                   blk.25.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 311/ 387]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 312/ 387]                   blk.25.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 313/ 387]                 blk.25.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 314/ 387]                   blk.26.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 315/ 387]                 blk.26.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 316/ 387]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 317/ 387]                   blk.26.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 318/ 387]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 319/ 387]                   blk.26.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 320/ 387]                 blk.26.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 321/ 387]                        output.weight - [ 4096, 151936,     1,     1], type =    f16, quantizing to q6_K .. size =  1187.00 MiB ->   486.86 MiB\n",
      "[ 322/ 387]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 323/ 387]               blk.26.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 324/ 387]               blk.26.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 325/ 387]                 blk.26.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 326/ 387]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 327/ 387]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 328/ 387]               blk.27.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 329/ 387]               blk.27.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 330/ 387]                 blk.27.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 331/ 387]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 332/ 387]                   blk.27.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 333/ 387]                 blk.27.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 334/ 387]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 335/ 387]                   blk.27.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 336/ 387]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 337/ 387]                   blk.27.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 338/ 387]                 blk.27.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 339/ 387]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 340/ 387]               blk.28.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 341/ 387]               blk.28.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 342/ 387]                 blk.28.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 343/ 387]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 344/ 387]                   blk.28.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 345/ 387]                 blk.28.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 346/ 387]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 347/ 387]                   blk.28.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 348/ 387]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 349/ 387]                   blk.28.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 350/ 387]                 blk.28.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 351/ 387]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 352/ 387]               blk.29.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 353/ 387]               blk.29.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 354/ 387]                 blk.29.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 355/ 387]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 356/ 387]                   blk.29.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 357/ 387]                 blk.29.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 358/ 387]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 359/ 387]                   blk.29.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 360/ 387]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 361/ 387]                   blk.29.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 362/ 387]                 blk.29.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 363/ 387]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 364/ 387]               blk.30.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 365/ 387]               blk.30.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 366/ 387]                 blk.30.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 367/ 387]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 368/ 387]                   blk.30.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 369/ 387]                 blk.30.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 370/ 387]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 371/ 387]                   blk.30.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 372/ 387]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 373/ 387]                   blk.30.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 374/ 387]                 blk.30.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 375/ 387]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 376/ 387]               blk.31.ffn_down.weight - [11008,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    86.00 MiB ->    35.27 MiB\n",
      "[ 377/ 387]               blk.31.ffn_gate.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 378/ 387]                 blk.31.ffn_up.weight - [ 4096, 11008,     1,     1], type =    f16, quantizing to q4_K .. size =    86.00 MiB ->    24.19 MiB\n",
      "[ 379/ 387]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 380/ 387]                   blk.31.attn_k.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 381/ 387]                 blk.31.attn_k.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 382/ 387]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 383/ 387]                   blk.31.attn_q.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 384/ 387]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
      "[ 385/ 387]                   blk.31.attn_v.bias - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 386/ 387]                 blk.31.attn_v.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
      "[ 387/ 387]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 14728.52 MB\n",
      "llama_model_quantize_internal: quant size  =  4540.59 MB\n"
     ]
    }
   ],
   "source": [
    "!pushd ../llama.cpp/; quantize ../notebooks/\"$model_dir\"/ggml-model-f16.gguf ../notebooks/\"$model_dir\"/ggml-model-Q4_K_M.gguf Q4_K_M; popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 387 tensors from qwen-2-7b/ggml-model-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = qwen-2-7b\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.72 B\n",
      "llm_load_print_meta: model size       = 4.43 GiB (4.93 BPW) \n",
      "llm_load_print_meta: general.name     = qwen-2-7b\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 30 '?'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   333.84 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4206.75 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size = 16384.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 16384.00 MiB, K (f16): 8192.00 MiB, V (f16): 8192.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =   296.75 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =  2128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    72.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1156\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'qwen2.attention.head_count': '32', 'general.name': 'qwen-2-7b', 'general.architecture': 'qwen2', 'qwen2.block_count': '32', 'qwen2.context_length': '32768', 'qwen2.attention.head_count_kv': '32', 'qwen2.embedding_length': '4096', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.feed_forward_length': '11008', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'general.file_type': '15', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '<|im_end|>' + '\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    f'{model_dir}/ggml-model-Q4_K_M.gguf',\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=32768,\n",
    "    chat_format='chatml',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2020 World Series was won by the Los Angeles Dodgers. They defeated the Tampa Bay Rays in six games to capture their first championship since 1988. The data in JSON format would look like this:\n",
      "```json\n",
      "{\n",
      "  \"year\": 2020,\n",
      "  \"champion\": \"Los Angeles Dodgers\",\n",
      "  \"runner-up\": \"Tampa Bay Rays\",\n",
      "  \"games_played\": 6\n",
      "}\n",
      "```\n",
      "Please note that this information is accurate as of my last update, which is up-to-date as of my knowledge cut-off in October 2021. If there have been any updates after that, you would need to check the"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1940.62 ms\n",
      "llama_print_timings:      sample time =      64.79 ms /   147 runs   (    0.44 ms per token,  2268.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1940.46 ms /    35 tokens (   55.44 ms per token,    18.04 tokens per second)\n",
      "llama_print_timings:        eval time =    3331.36 ms /   146 runs   (   22.82 ms per token,    43.83 tokens per second)\n",
      "llama_print_timings:       total time =    6635.12 ms /   181 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " latest sports data sources."
     ]
    }
   ],
   "source": [
    "generator = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n",
    "    ],\n",
    "    # response_format={\n",
    "    #     \"type\": \"json_object\",\n",
    "    #     \"schema\": {\n",
    "    #         \"type\": \"object\",\n",
    "    #         \"properties\": {\"team_name\": {\"type\": \"string\"}},\n",
    "    #         \"required\": [\"team_name\"],\n",
    "    #     },\n",
    "    # },\n",
    "    temperature=0.5,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "next(generator)\n",
    "for tok in generator:\n",
    "    if not tok['choices']:\n",
    "        break\n",
    "\n",
    "    choice = tok['choices'][0]\n",
    "    if not choice['delta']:\n",
    "        break\n",
    "\n",
    "    print(choice[\"delta\"][\"content\"], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.tokenizer_.decode([llm.token_eos()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./qwen-2-4b/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151645"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id\n",
    "tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151645]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.tokenizer_.decode([39])\n",
    "llm.tokenizer_.encode(tokenizer.eos_token, add_bos=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
