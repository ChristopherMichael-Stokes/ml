{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c244509-2096-4953-8d1a-1b891e31b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb795058-c63a-4d4d-aab4-f36851319310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_device('cuda')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch, 'mps') and torch.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a0696-b988-414c-acd3-e161b0d700e5",
   "metadata": {},
   "source": [
    "# Mixture of depths dynamic computation optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a56c9f9-46ec-4eb6-aa78-72766418b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tensor(name: str, value: torch.tensor):\n",
    "    \"\"\"This thing is extremely useful for making sure we are doing the correct\n",
    "    operations (on the correct values!!) as my tensor calculus is very error prone.\"\"\"\n",
    "    print(f'{name}\\n{\"-\"*20}\\n{value.shape}\\n{value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca3b31-485b-42d9-9772-a765e7876591",
   "metadata": {},
   "source": [
    "## 1. Figuring out the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea829f13-5db5-4363-a84c-b46c1113dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42)\n",
    "\n",
    "## Network hyperparameters\n",
    "B = 256 # Batch size\n",
    "S = 4 # Sequence length\n",
    "h = 3 # Hidden / embedding dimension\n",
    "#   ^ only using small, unique, and values > 1 for easy debugging\n",
    "## MoD hyperparameters\n",
    "# Control parameter for how many tokens must go through the layer / self attention,\n",
    "# C tokens are passed through the decoder layer and S - C tokens are residual connection only \n",
    "C = 2 # Must be 1 < C <= S\n",
    "# Compute efficiency is obtained from lesser C values while higher values preserve evaluation performance\n",
    "# However there exists a pareto optimal combination of both as proven in the paper through ablation studies\n",
    "\n",
    "## Batch inputs\n",
    "X = torch.randn(B, S, h, device=device)\n",
    "\n",
    "## Network subsection\n",
    "# Full network is simply [EncoderLayer(), PositionalEncoding(), [DecoderLayer(), ...], LMHead()]\n",
    "# Input to the network is an integer tensor of shape [Batch length, Sequence length, 1]\n",
    "# Output is a float tensor of shape [Batch length, Sequence length, Vocab size]*\n",
    "## *Though we generally only care about the output at the last sequence index so can be optimised to \n",
    "## [Batch length, 1, Vocab size]\n",
    "\n",
    "## Standard transformer decoder block layer pseudocode (see figure 1 in paper)\n",
    "# def layer(x: torch.tensor) -> torch.tensor:\n",
    "#     x_in = x.copy()\n",
    "#     x = LayerNorm(x)\n",
    "#     x = SelfAttention(x) \n",
    "#     x_in = torch.concat((x_in, x), dim=1)\n",
    "#     x = LayerNorm(x_in)\n",
    "#     x = Linear(x)\n",
    "#     x = GLU(x) # ReLU originally but modern SoTA use GLU derivatves, i.e. swiGLU for llama2\n",
    "#     # See https://arxiv.org/abs/2002.05202 (check the conclusion :p).\n",
    "#     x = Linear(x)\n",
    "#     return x\n",
    "layer  = lambda x_: x_ # No-op, just return input for visual debugging\n",
    "\n",
    "\n",
    "# Tokens can either be omitted from the layer computations and only concatenated to the output, \n",
    "# or go through the computation + have a residial connection by multiplication with the last ffnn\n",
    "router_l = nn.Linear(h, 1, device=device) # Routing head of current layer\n",
    "X_l = X.clone() # Inputs to current decoder layer\n",
    "\n",
    "\n",
    "## Mixture of depths layer computations, # def mod_layer(X_l: torch.tensor) -> torch.tensor: \n",
    "R_l = router_l(X_l) # Router activations\n",
    "r_l, r_l_i = R_l.topk(C, dim=1, sorted=True) # Find top C tokens to pass through network\n",
    "r_l_i.squeeze_() # Make shape [Batch, C]\n",
    "\n",
    "# Create a multihot mask from the list of routing indices (r_l_i)\n",
    "X_tilde_mask = torch.zeros(r_l_i.size(0), S, device=X.device).scatter_(1, r_l_i, 1.).type(torch.BoolTensor).unsqueeze(-1).to(device)\n",
    "# Select the token embeddings from the layer input (X_l) based on the ranking indices (r_l_i) via the mask\n",
    "X_tilde = torch.masked_select(X_l, X_tilde_mask).view(B, C, h)\n",
    "# took me ages to find ^ this function, masked select works exactly the same as boolean indexing in pandas\n",
    "# but in higher dimensional space\n",
    "\n",
    "# Notice how the r_l term is in the X_routed operation, this allows backprop to train the router layer weights\n",
    "X_routed = r_l * layer(X_tilde) + X_tilde\n",
    "X_unrouted = torch.masked_select(X_l, ~X_tilde_mask).view(B, S-C, h) # The remainder of tokens for skip  connection\n",
    "\n",
    "# # The order / sorting of the final concat does not matter at all thanks to our friend positional encoding.\n",
    "X_l1 = torch.concat((X_routed, X_unrouted), dim=1) # Careful to concat along the sequence dimension\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    print(f\"Batch size:       {B}\\nSequence length:  {S}\\nHidden dimension: {h}\", end=\"\\n\\n\")\n",
    "    print_tensor(\"Layer inputs (X_l)\", X_l)\n",
    "    print_tensor(\"R_l\", R_l)\n",
    "    print_tensor(\"r_l\", r_l)\n",
    "    print_tensor(\"r_l_i\", r_l_i)\n",
    "    print_tensor(\"X_tilde_mask\", X_tilde_mask)\n",
    "    print_tensor(\"X_tilde\", X_tilde)\n",
    "    print_tensor(\"X_routed\", X_routed)\n",
    "    print_tensor(\"Layer outputs (X_l1)\", X_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114d636-2902-4a42-9c34-2c1b72beda67",
   "metadata": {},
   "source": [
    "## 2. Wrapping in a basic GPT like transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "504818cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([2, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, hidden_dim)\n",
    "        self.gate = nn.Linear(dim, hidden_dim)\n",
    "        self.transform = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transform(self.linear(x) * torch.sigmoid(self.gate(x)))\n",
    "\n",
    "\n",
    "B = 2\n",
    "S = 10\n",
    "n_embed = 768\n",
    "n_head = 12\n",
    "X = torch.randn(B, S, n_embed, device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "swiglu = SwiGLU(dim=n_embed, hidden_dim=4*n_embed).to(device)\n",
    "output = swiglu(X)\n",
    "\n",
    "\n",
    "print(\"Output Shape:\", output.shape)  # Should match [batch_size, out_features]\n",
    "del X, layer, output\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e0027e8-2fca-4ac9-ba4c-e15aef0ee4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "--------------------\n",
      "torch.Size([2, 10, 768])\n",
      "tensor([[[ 0.1940,  2.1614, -0.1721,  ..., -0.6821,  0.7974, -0.8484],\n",
      "         [ 0.8574,  0.4992,  0.1359,  ..., -1.0946, -1.1731, -0.4472],\n",
      "         [-1.2405, -0.1784,  0.4220,  ...,  0.9749,  0.5732,  1.3790],\n",
      "         ...,\n",
      "         [ 0.5391,  0.4001,  1.0236,  ..., -1.0001, -1.1552, -0.6955],\n",
      "         [ 1.7624, -1.2477,  1.2913,  ...,  0.0937, -0.7382,  0.2442],\n",
      "         [-0.1452, -2.5195,  1.4329,  ...,  0.5431,  0.6378,  0.3826]],\n",
      "\n",
      "        [[-0.4443,  1.8408,  1.2662,  ...,  0.6949,  0.3456, -0.6991],\n",
      "         [ 0.0722,  0.7010,  2.5725,  ...,  0.8067,  0.1701,  0.7526],\n",
      "         [-0.1311, -1.1856,  0.5139,  ...,  0.2042, -1.1267, -0.0607],\n",
      "         ...,\n",
      "         [-0.5234, -1.6324, -0.5524,  ..., -0.5802,  0.4249, -1.7961],\n",
      "         [-2.5483,  0.1865,  1.3971,  ..., -0.1381, -0.5499, -1.7784],\n",
      "         [ 0.9432,  0.6944, -0.2105,  ..., -1.5116, -0.4728, -1.5385]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "output\n",
      "--------------------\n",
      "torch.Size([2, 10, 768])\n",
      "tensor([[[ 0.5763,  2.3308,  0.4158,  ..., -0.6079,  1.2515, -0.1623],\n",
      "         [ 0.5168,  0.9591, -0.4305,  ..., -0.8670, -1.2075, -0.4472],\n",
      "         [-1.5188, -0.8171,  0.7394,  ...,  1.0702,  0.5729,  1.8522],\n",
      "         ...,\n",
      "         [ 1.0254, -0.3479,  0.9663,  ..., -0.5255, -1.4318, -0.3986],\n",
      "         [ 1.3942, -1.7506,  0.9910,  ...,  0.4451, -0.8025,  0.0948],\n",
      "         [-0.4424, -1.7988,  1.9915,  ...,  0.2858, -0.1835,  0.1385]],\n",
      "\n",
      "        [[-0.1764,  2.0098,  1.4264,  ...,  1.2086,  0.3144, -0.0932],\n",
      "         [-0.1397,  1.1522,  2.2256,  ...,  1.2691, -0.1571,  0.6691],\n",
      "         [-0.1617, -1.8057,  0.7834,  ...,  0.1471, -0.9265,  0.5018],\n",
      "         ...,\n",
      "         [-0.1602, -1.9762, -0.4797,  ..., -0.2814,  0.0914, -1.8178],\n",
      "         [-2.7123, -0.7608,  0.9743,  ..., -0.0971, -0.5050, -2.1973],\n",
      "         [ 0.6946,  1.2903,  0.7016,  ..., -1.7035, -1.7208, -1.7541]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"Careful there are a few versions of GeLU, this one is the exact one used by OpenAI\"\"\"\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim, hidden_dim)\n",
    "        self.gate = nn.Linear(dim, hidden_dim)\n",
    "        self.transform = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transform(self.linear(x) * torch.sigmoid(self.gate(x)))\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int, swiglu: bool=True):\n",
    "        super().__init__()\n",
    "        if swiglu:\n",
    "            self.feed_forward = SwiGLU(n_embed, 4 * n_embed)\n",
    "        else:\n",
    "            self.feed_forward = nn.Sequential(\n",
    "                [\n",
    "                    nn.Linear(n_embed, 4 * n_embed),\n",
    "                    NewGELU(),\n",
    "                    nn.Linear(4 * n_embed, n_embed),\n",
    "                ]\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.feed_forward(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int, h_head: int):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(n_embed)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=n_embed, num_heads=n_head)\n",
    "        self.ln_2 = nn.LayerNorm(n_embed)\n",
    "        self.mlp = MLP(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_t = self.ln_1(x) # keep layer norm full precision\n",
    "\n",
    "        with torch.autocast('cuda', torch.bfloat16):\n",
    "            x = x + self.attn(x_t, x_t, x_t, need_weights=False)[0]\n",
    "            x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "B = 2\n",
    "S = 10\n",
    "n_embed = 768\n",
    "n_head = 12\n",
    "X = torch.randn(B, S, n_embed, device=device)\n",
    "\n",
    "layer = DecoderBlock(n_embed, n_head).to(device)\n",
    "# layer = nn.TransformerDecoderLayer(d_model=h, nhead=nhead, activation=F.glu, batch_first=True)\n",
    "\n",
    "out = layer(X)\n",
    "print_tensor(\"input\", X)\n",
    "print_tensor(\"output\", out)\n",
    "\n",
    "del X, layer, out\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f0e7d34-0402-44bb-bd77-6b9546a8f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "--------------------\n",
      "torch.Size([2, 10, 768])\n",
      "tensor([[[-1.6291,  1.3206,  1.5349,  ...,  0.6512,  0.2013, -0.8091],\n",
      "         [-0.6031,  0.7363,  1.3001,  ..., -0.9398,  0.1413, -0.2003],\n",
      "         [ 1.2939,  1.2201,  0.1389,  ...,  0.2965, -0.7913, -0.6119],\n",
      "         ...,\n",
      "         [-0.4256,  2.3303, -1.0826,  ..., -0.6241,  3.0210, -2.6685],\n",
      "         [ 2.5794,  1.4523,  0.7480,  ...,  0.0672,  0.0753, -1.0983],\n",
      "         [ 0.7906, -0.3601,  3.4060,  ..., -0.0181,  0.6491, -1.7173]],\n",
      "\n",
      "        [[ 1.5160, -1.0064, -0.0648,  ...,  0.0188,  0.1854, -1.1085],\n",
      "         [ 0.6835,  1.1110, -0.4112,  ...,  0.3753, -1.6994,  1.1767],\n",
      "         [-0.7526,  0.2568,  0.6397,  ..., -3.7622,  0.1098,  0.4100],\n",
      "         ...,\n",
      "         [-1.9752, -0.6943,  0.3540,  ..., -0.1650,  1.4039,  0.8064],\n",
      "         [-0.2743, -0.2561,  0.4554,  ...,  0.2385, -0.4198, -1.0945],\n",
      "         [ 0.4451,  0.5967,  1.3188,  ...,  0.5855, -0.0664,  0.9142]]],\n",
      "       device='cuda:0')\n",
      "\n",
      "output\n",
      "--------------------\n",
      "torch.Size([2, 10, 768])\n",
      "tensor([[[ 2.4060,  2.2260,  0.0677,  ...,  0.5363, -1.5599, -1.6261],\n",
      "         [ 2.1904,  0.8448, -0.4459,  ..., -3.7007, -1.4522,  1.6811],\n",
      "         [ 0.9915, -0.8733, -2.6139,  ..., -1.3415, -0.4120, -1.1646],\n",
      "         ...,\n",
      "         [ 0.9809,  0.7913,  1.2355,  ...,  1.2014,  2.3584, -0.1796],\n",
      "         [ 0.2807, -0.2421,  0.8969,  ..., -1.1659,  2.2832,  0.4032],\n",
      "         [ 2.5794,  1.4523,  0.7480,  ...,  0.0672,  0.0753, -1.0983]],\n",
      "\n",
      "        [[ 1.1420,  1.7966, -0.8044,  ...,  0.7621, -2.6838,  1.6601],\n",
      "         [-2.7988, -1.8001, -0.5380,  ...,  2.6635, -4.3907,  0.7811],\n",
      "         [-2.5525, -0.8517,  0.5190,  ..., -0.1105,  1.7427,  1.0363],\n",
      "         ...,\n",
      "         [-0.3857,  0.7222, -0.7667,  ..., -0.3261, -0.4610, -0.6766],\n",
      "         [ 1.5813, -0.3320, -1.5860,  ..., -0.0448,  1.2706,  0.5514],\n",
      "         [-0.1793, -0.3020, -1.5083,  ..., -1.6121,  0.4315, -1.7025]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MoDDecoderLayer(nn.Module):\n",
    "    def __init__(self, n_embed: int, n_head: int, C: int):\n",
    "        super().__init__()\n",
    "        self.h = n_embed\n",
    "        self.C = C\n",
    "        self.router = nn.Linear(n_embed, 1)\n",
    "        # self.layer = nn.TransformerDecoderLayer(d_model=h, nhead=nhead, activation=F.glu)#, batch_first=True)\n",
    "        self.layer = DecoderBlock(n_embed, n_head)\n",
    "\n",
    "    def forward(self, X) -> torch.Tensor:\n",
    "        B, S, h = X.shape\n",
    "        ## Mixture of depths layer computations, # def mod_layer(X_l: torch.tensor) -> torch.tensor: \n",
    "        R = self.router(X) # Router activations\n",
    "        r, r_i = R.topk(self.C, dim=1, sorted=True) # Find top C tokens to pass through network\n",
    "        r_i = r_i.squeeze() # Make shape [Batch, C]\n",
    "        \n",
    "        # Create a multihot mask from the list of routing indices (r_l_i)\n",
    "        X_tilde_mask = torch.zeros(r_i.size(0), S, device=X.device).scatter(1, r_i, 1.).type(torch.bool).unsqueeze(-1)\n",
    "\n",
    "        ## ^^ issue here with that thing, might need to register buffer\n",
    "        # Select the token embeddings from the layer input (X_l) based on the ranking indices (r_l_i) via the mask\n",
    "        X_tilde = torch.masked_select(X, X_tilde_mask).view(B, self.C, h)\n",
    "        # took me ages to find ^ this function, masked select works exactly the same as boolean indexing in pandas\n",
    "        # but in higher dimensional space\n",
    "        \n",
    "        # Notice how the r_l term is in the X_routed operation, this allows backprop to train the router layer weights\n",
    "\n",
    "        #### TODO: fix the stuf with the layer forward pass, not sure what to do here\n",
    "        X_routed = r * self.layer(X_tilde) + X_tilde\n",
    "\n",
    "\n",
    "        \n",
    "        X_unrouted = torch.masked_select(X, ~X_tilde_mask).view(B, S-C, h) # The remainder of tokens for skip  connection\n",
    "        \n",
    "        # The order / sorting of the final concat does not matter at all thanks to our friend positional encoding.\n",
    "        X_l1 = torch.concat((X_routed, X_unrouted), dim=1) # Careful to concat along the sequence dimension\n",
    "\n",
    "        return X_l1\n",
    "\n",
    "\n",
    "B = 2\n",
    "S = 10\n",
    "C = 5\n",
    "n_embed = 768\n",
    "n_head = 12\n",
    "X = torch.randn(B, S, n_embed, device=device)\n",
    "\n",
    "layer = MoDDecoderLayer(n_embed, n_head, C).to(device)\n",
    "# layer = nn.TransformerDecoderLayer(d_model=h, nhead=nhead, activation=F.glu, batch_first=True)\n",
    "\n",
    "out = layer(X)\n",
    "print_tensor(\"input\", X)\n",
    "print_tensor(\"output\", out)\n",
    "del X, layer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09d05c-f3f6-4320-9c04-b37a57b92e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ea67c-319a-4170-ac57-c139f06e6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find pytorch glu implementation https://pytorch.org/cppdocs/api/function_namespaceat_1aa10cf0aaff07f0a75dbfa51f168f563d.html#_CPPv4N2at3gluERKN2at6TensorE7int64_t\n",
    "# -- looks like I need to search the codebase as pytorch uses a compiled function https://github.com/pytorch/pytorch/blob/7c23fed12c24ad7d635b0aa7af08449c9510375c/torch/nn/functional.py#L1514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a31f9e-d578-4ea3-ac14-9d3b8091be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f892595-cf7f-4a52-9980-4954507cbc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Shape: torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomModule(nn.Module):\n",
    "    def __init__(self, L, D):\n",
    "        super(CustomModule, self).__init__()\n",
    "        # L and D can be set as per model requirements or input feature sizes\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        # Assume some initial data tensor, possibly as a parameter or buffer if fixed\n",
    "        self.data = nn.Parameter(torch.randn(1, L, D))\n",
    "\n",
    "    def forward(self, indices):\n",
    "        # Obtain batch size from indices\n",
    "        N, M = indices.shape\n",
    "\n",
    "        # Assuming data is (1, L, D) and needs to be expanded to (N, L, D)\n",
    "        data_expanded = self.data.expand(N, -1, -1)\n",
    "\n",
    "        # Create a range for the N dimension to use in advanced indexing\n",
    "        batch_indices = torch.arange(N, device=indices.device).view(-1, 1).expand(-1, M)\n",
    "\n",
    "        # Use advanced indexing to get the NxMxD tensor\n",
    "        result = data_expanded[batch_indices, indices]\n",
    "\n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "N, M, L, D = 3, 4, 5, 2\n",
    "indices = torch.randint(low=0, high=L, size=(N, M))\n",
    "\n",
    "model = CustomModule(L, D)\n",
    "result = model(indices)\n",
    "\n",
    "print(\"Result Shape:\", result.shape)  # Should be (N, M, D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81a43f-c549-4ee3-a095-9a2cdcc89b72",
   "metadata": {},
   "source": [
    "## 3. Training on fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edeab3-20c0-41d0-8481-87ead74d6d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4b04c-40d0-40b2-84ca-d90699fbe011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170964d2-5390-496b-9cc7-3e3225642e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49b207c1-c3f7-4a6b-ba5f-2f435d47a1fc",
   "metadata": {},
   "source": [
    "## 4. Training on sample real data\n",
    "TinyShakespeare + tiktoken tokenisation Ã  la Kaparthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedab303-cf68-42e7-9da6-2c2da9fa3815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a2164-8a97-4b30-988c-3f9f61783c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ea4cb04",
   "metadata": {},
   "source": [
    "## 5. Evaluation / ablations\n",
    "\n",
    "1 idea to choose c values is to train with no c constraint and look at activation distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd79b7-e3a7-4341-818b-3d713e3ad642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95522d2d-592c-4080-a0e0-a683ba47ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to combine the one-hot vectors in the last dimension to get 2x2x1 instead of 2x2x4\n",
    "torch.zeros(r_l_i.squeeze().size(0), S).scatter_(1, r_l_i.squeeze(), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6ccb9-e2ed-4bfb-9f0a-47438654f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_l = router(X_l)\n",
    "r_l, r_l_i = R_l.topk(C, dim=1, sorted=True)\n",
    "r_l_i.squeeze_() # Make shape [Batch, C]\n",
    "\n",
    "X_tilde_mask = torch.zeros(r_l_i.size(0), S).scatter_(1, r_l_i, 1.).type(torch.BoolTensor).unsqueeze(-1)\n",
    "X_tilde = torch.masked_select(X_l, X_tilde_mask).view(B, C, h)\n",
    "\n",
    "X_routed = r_l * layer(X_tilde) + X_tilde\n",
    "X_unrouted = torch.masked_select(X_l, ~X_tilde_mask).view(B, S-C, h) # The remainder of tokens for skip  connection\n",
    "\n",
    "X_l1 = torch.concat((X_routed, X_unrouted), dim=1)\n",
    "\n",
    "\n",
    "print_tensor(\"X_l\", X_l)\n",
    "print_tensor(\"R_l\", R_l)\n",
    "print_tensor(\"r_l\", r_l)\n",
    "print_tensor(\"r_l_i\", r_l_i)\n",
    "print_tensor(\"X_tilde_mask\", X_tilde_mask)\n",
    "print_tensor(\"X_tilde\", X_tilde)\n",
    "print_tensor(\"X_routed\", X_routed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3293cda-19c1-486c-9d02-34c8b7fafa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat((X_routed, X_unrouted), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438df9c-babc-487a-883d-3e4b5a086298",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1, 4, 1, 0, 5, 2])\n",
    "labels = labels.unsqueeze(0)\n",
    "target = torch.zeros(labels.size(0), 15).scatter_(1, labels, 1.)\n",
    "labels.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8839d8-f5fa-42de-a29a-d0096c5193f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2678cf-c90c-4ffe-9f92-647f7097342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.randn(10, 6000)\n",
    "idx = torch.topk(scores, 6000, dim=1, sorted=True)\n",
    "out = torch.gather(scores, dim=1, index=idx.indices)\n",
    "idx.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19c5e0-f725-4b4d-a842-9e8db4fd12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l, r_l_i, X_l[0, :], X_tilde[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea6f6b-8c72-443b-9e93-f5e91187298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l.r_l_i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5713579-6c1c-49f5-8ad8-646000c1af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.masked_select(X_l, ~X_tilde_mask.unsqueeze(-1)).view(B, (S + 1) - C, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228334f3-14d3-41e3-867b-0d257efb826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(384).view(2, 2, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed852a-5422-438a-9135-11a1ffbc3c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf2035-ebac-42fe-bec4-bee5d844d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc547378-f9d5-42f3-ad99-ce80ab3deb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f7214-0252-48d3-bf15-58bffef21915",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l.shape, X_tilde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5612e6b-5a6b-460f-86fc-c1c7ef5ec2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tilde * r_l.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53588adf-feb5-4118-a59d-47700f340d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a0027-cca2-4c6d-83bb-3444563b4ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff141a8b-4e16-4fdf-a50c-c625b6f9799f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884f7cc-8d70-42b0-88ef-dd613bff788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l.shape, i.shape, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c31ad-d0f2-4a9e-9c53-baefb0095345",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tilde.shape, x_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bec464-fad3-48eb-9275-8b655733fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.masked_select(X_l, x_tilde.unsqueeze(-1)).view(B, C, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee8cf9-a3e9-4279-bdb9-697ce57aac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tilde.type(torch.BoolTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b887b71-af07-4ec1-893a-bf2fba35fe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3a831-e968-4573-a68b-ebd2a140ebd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad547d-ebe9-43a8-9018-fdafab3d1d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db02d184-c99e-4ad0-a368-dbef0de306a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba208322-aeec-4bab-bfb7-5579bcb29918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bead9-4920-4935-8e7d-84c7254c8fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd17fa-d414-46c2-aa1f-023ed1413f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afc0fa-bcab-4622-b882-4a73a74d63bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ed21a-8f9b-47a6-a103-3ac3f763cdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543a40f-6ff0-4672-8058-31645e425a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572315b-9f7f-4c1f-a888-f4198d3a7a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786770f-044a-4246-b208-34b4288f23fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26c69b-3734-4619-81d3-cadcbcd86c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4e490-aa1b-4068-a02a-2aed1c67730b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bddcc8-0830-4388-b9fc-81846b6e4bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6b157-eb2b-44b8-aca5-73232d33eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(i, S).shape, x_routed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3192b96-eef0-41cd-93c7-a9b9e61b2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.scatter(-1, i, x)\n",
    "X.clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10524332-c62e-45d9-b3c8-37a9961969d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.scatter(-1, x_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f879d6-7894-4390-aa98-f0e410531d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6cf8e-063f-490d-abdf-6a48f924366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "R.shape, x.shape, minc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7746ae92-1268-4c74-a5f0-d25d41f76681",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gt(R, minc.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb3074-75ff-4543-bdcc-83e696fb318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbece0c2-1e53-4c06-8b42-737f4aa47246",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac68420-38cc-4e3e-96ca-2d65b2ebea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525978b6-93be-41bd-9cdb-b26ab937302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c7ad6-a00a-4dba-957d-2659eeb32a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R[-1] > pbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f798d05-c51c-4d0e-aaeb-c12c022c48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "1  - (C/S) # routing the top 75th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c1de9-3d9a-4085-bd5f-910eadd66ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "R.topk(C, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a5bff-be8f-4d43-8749-8b400c7c91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_c.indices.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761bd34-8dac-4029-a8fa-f86faefda1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.topk(1, dim=2).values.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28288985-53ab-41b6-a5f7-d4041cfd1d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat((X,X), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c8472-cc55-4315-bc0f-bf665b24c093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3d985-b600-4583-a044-68a5147809b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
