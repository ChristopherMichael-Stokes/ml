{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-03-22T23:37:49.841934Z",
     "iopub.status.busy": "2024-03-22T23:37:49.841934Z",
     "iopub.status.idle": "2024-03-22T23:37:51.336122Z",
     "shell.execute_reply": "2024-03-22T23:37:51.335617Z",
     "shell.execute_reply.started": "2024-03-22T23:37:49.841934Z"
    },
    "id": "Zxlo42ldpvi8",
    "outputId": "985d8ff0-3cf0-45fc-f81c-54e2f7ce30be",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\chris\\miniconda3\\envs\\ai\\lib\\site-packages (0.27.2)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\chris\\miniconda3\\envs\\ai\\lib\\site-packages (0.43.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\chris\\miniconda3\\envs\\ai\\lib\\site-packages (0.1.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
      "ERROR: No matching distribution found for faiss-gpu\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate bitsandbytes langchain faiss-gpu dspy pypdf sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:20:02.895219Z",
     "iopub.status.busy": "2024-03-23T15:20:02.895219Z",
     "iopub.status.idle": "2024-03-23T15:20:02.915479Z",
     "shell.execute_reply": "2024-03-23T15:20:02.914974Z",
     "shell.execute_reply.started": "2024-03-23T15:20:02.895219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-03-23T15:20:04.683963Z",
     "iopub.status.busy": "2024-03-23T15:20:04.683963Z",
     "iopub.status.idle": "2024-03-23T15:20:05.539422Z",
     "shell.execute_reply": "2024-03-23T15:20:05.538916Z",
     "shell.execute_reply.started": "2024-03-23T15:20:04.683963Z"
    },
    "id": "SHIRHo1Qrf_T",
    "outputId": "479a96e1-8d07-4ca4-9af4-acd2a1560331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\chris\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:20:06.518107Z",
     "iopub.status.busy": "2024-03-23T15:20:06.518107Z",
     "iopub.status.idle": "2024-03-23T15:20:09.875770Z",
     "shell.execute_reply": "2024-03-23T15:20:09.875266Z",
     "shell.execute_reply.started": "2024-03-23T15:20:06.518107Z"
    },
    "id": "1veF3JwNpwuY"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer, AutoConfig\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e225606e93e249a981530828d2ba9727",
      "bbf66deef35b4a969d616d1094c84f39",
      "2c1780a44ba24430b646cf498359fedd",
      "485ca683a09043ab991da114ff9c299b",
      "b54b9ab58688455d83c3ea53842f5a93",
      "f6748edf2a414b9f9aba069de012406f",
      "44a85744e1a04120927f20b9091c2157",
      "f4d7eccb740a4fa5a972cd56966e4ca3",
      "53c2815db3554e369d7f7892a05459c4",
      "aac14b2d2f6e4e7ca6f290ad3b443ea5",
      "ffbfc9148193426a9eb7c6bd29ceea19"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-03-23T15:20:09.877272Z",
     "iopub.status.busy": "2024-03-23T15:20:09.876771Z",
     "iopub.status.idle": "2024-03-23T15:20:18.775089Z",
     "shell.execute_reply": "2024-03-23T15:20:18.774582Z",
     "shell.execute_reply.started": "2024-03-23T15:20:09.877272Z"
    },
    "id": "rm2ukBQTrBOh",
    "outputId": "d27cc275-ee91-4888-ac92-4c57d785ce55"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acac1fbaf164eb58d07efe1b6c35737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(model_name)\n",
    "model_config.hidden_act = 'gelu_pytorch_tanh'\n",
    "model_config.hidden_activation = 'gelu_pytorch_tanh'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, config=model_config, device_map='auto', quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:20:18.776088Z",
     "iopub.status.busy": "2024-03-23T15:20:18.775589Z",
     "iopub.status.idle": "2024-03-23T15:20:18.778914Z",
     "shell.execute_reply": "2024-03-23T15:20:18.778408Z",
     "shell.execute_reply.started": "2024-03-23T15:20:18.776088Z"
    },
    "id": "MzauJTg2zwlh"
   },
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-03-23T15:20:18.779414Z",
     "iopub.status.busy": "2024-03-23T15:20:18.779414Z",
     "iopub.status.idle": "2024-03-23T15:20:18.797829Z",
     "shell.execute_reply": "2024-03-23T15:20:18.797325Z",
     "shell.execute_reply.started": "2024-03-23T15:20:18.779414Z"
    },
    "id": "sEZjr8Vjs2nr",
    "outputId": "c2ed58f2-c640-493f-9615-3914da2ffbdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  7310, 26448]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer('example texts', padding=True, return_tensors='pt', )\n",
    "x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-03-22T23:37:58.940282Z",
     "iopub.status.busy": "2024-03-22T23:37:58.939277Z",
     "iopub.status.idle": "2024-03-22T23:38:36.359083Z",
     "shell.execute_reply": "2024-03-22T23:38:36.359083Z",
     "shell.execute_reply.started": "2024-03-22T23:37:58.940282Z"
    },
    "id": "qgwkwt9Jt8mA",
    "outputId": "652c4ca6-2d6d-43fe-d0cd-9c57234d9ada"
   },
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=x.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    attention_mask=x.attention_mask,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-03-22T23:38:36.360597Z",
     "iopub.status.busy": "2024-03-22T23:38:36.360094Z",
     "iopub.status.idle": "2024-03-22T23:38:36.365223Z",
     "shell.execute_reply": "2024-03-22T23:38:36.364719Z",
     "shell.execute_reply.started": "2024-03-22T23:38:36.360597Z"
    },
    "id": "t5RMCbGquDbS",
    "outputId": "49b293d1-a25f-46d8-e734-06091a209853"
   },
   "source": [
    "pprint(tokenizer.batch_decode(outputs))\n",
    "del outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 39
    },
    "execution": {
     "iopub.execute_input": "2024-03-23T15:20:47.916348Z",
     "iopub.status.busy": "2024-03-23T15:20:47.916348Z",
     "iopub.status.idle": "2024-03-23T15:20:48.132776Z",
     "shell.execute_reply": "2024-03-23T15:20:48.132269Z",
     "shell.execute_reply.started": "2024-03-23T15:20:47.916348Z"
    },
    "id": "hUWKMU8f0PYa",
    "outputId": "80660b90-98df-433a-8f0e-c30baf154a07"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:25:32.537062Z",
     "iopub.status.busy": "2024-03-23T15:25:32.536563Z",
     "iopub.status.idle": "2024-03-23T15:25:32.541416Z",
     "shell.execute_reply": "2024-03-23T15:25:32.540916Z",
     "shell.execute_reply.started": "2024-03-23T15:25:32.537062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('c:/Users/chris/repos/ml/notebooks/data/4 Skills and qualities table.pdf'),\n",
       " WindowsPath('c:/Users/chris/repos/ml/notebooks/data/CS_Behaviours_2018.pdf'),\n",
       " WindowsPath('c:/Users/chris/repos/ml/notebooks/data/Sophia+CV+2021.pdf')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs = list(Path('./data').absolute().iterdir())\n",
    "pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:25:32.999445Z",
     "iopub.status.busy": "2024-03-23T15:25:32.999445Z",
     "iopub.status.idle": "2024-03-23T15:25:35.161836Z",
     "shell.execute_reply": "2024-03-23T15:25:35.161322Z",
     "shell.execute_reply.started": "2024-03-23T15:25:32.999445Z"
    },
    "id": "dALR0CIB4JWF"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_instruction=\"Represent this sentence for searching relevant passages: \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:25:35.163334Z",
     "iopub.status.busy": "2024-03-23T15:25:35.162835Z",
     "iopub.status.idle": "2024-03-23T15:25:35.166917Z",
     "shell.execute_reply": "2024-03-23T15:25:35.166409Z",
     "shell.execute_reply.started": "2024-03-23T15:25:35.163334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\chris\\\\repos\\\\ml\\\\notebooks\\\\data\\\\4 Skills and qualities table.pdf'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{pdfs[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:25:35.167416Z",
     "iopub.status.busy": "2024-03-23T15:25:35.167416Z",
     "iopub.status.idle": "2024-03-23T15:25:35.850218Z",
     "shell.execute_reply": "2024-03-23T15:25:35.849712Z",
     "shell.execute_reply.started": "2024-03-23T15:25:35.167416Z"
    },
    "id": "Ys4gXC_f4QYX"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "pages = [PyPDFLoader(f\"{pdf}\").load_and_split(text_splitter) for pdf in pdfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:25:36.958463Z",
     "iopub.status.busy": "2024-03-23T15:25:36.957962Z",
     "iopub.status.idle": "2024-03-23T15:26:13.775312Z",
     "shell.execute_reply": "2024-03-23T15:26:13.774803Z",
     "shell.execute_reply.started": "2024-03-23T15:25:36.958463Z"
    },
    "id": "4v0LwX0k6R0X"
   },
   "outputs": [],
   "source": [
    "faiss_index = FAISS.from_documents(pages[0], embedding_model)\n",
    "\n",
    "for page in pages[1:]:\n",
    "    faiss_index.merge_from(FAISS.from_documents(page, embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-03-23T15:26:25.343923Z",
     "iopub.status.busy": "2024-03-23T15:26:25.343417Z",
     "iopub.status.idle": "2024-03-23T15:26:25.477801Z",
     "shell.execute_reply": "2024-03-23T15:26:25.477298Z",
     "shell.execute_reply.started": "2024-03-23T15:26:25.343923Z"
    },
    "id": "gFTn4tJI6RCA",
    "outputId": "cf6ab193-5d76-4549-803d-228e481d2def"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: • Role-play\n",
      "• Oral briefing\n",
      "• Assessment centre. \n",
      " \n",
      "Depending on the type of assessment, for \n",
      "example in an application form or at an \n",
      "interview, you may be asked to give examples \n",
      "of when you have demonstrated a particular \n",
      "behaviour.This might be at work or somewhere else \n",
      "such as work experience, volunteering or in \n",
      "connection with a hobby.\n",
      "Alternatively, you may be asked how you would \n",
      "behave in a given situation, for example by \n",
      "using Situational Judgement Tests or through \n",
      "situational interview questions. You could \n",
      "also be asked to demonstrate a behaviour \n",
      "in real-time, for example during a structured \n",
      "behavioural assessment.\n",
      "Your behaviours may be assessed alongside \n",
      "other elements of the Success Profile to get \n",
      "a more rounded picture of your suitability for \n",
      "the role. The job description will outline the \n",
      "elements required for the role and the selection \n",
      "method(s) that will be used. \n",
      "The Civil Service is a diverse and inclusive \n",
      "workplace and we want to help you\n",
      "2: The Civil Service is a diverse and inclusive \n",
      "workplace and we want to help you \n",
      "demonstrate your full potential whatever type \n",
      "of assessment is used. If you require any \n",
      "reasonable adjustments to our recruitment \n",
      "process please let the recruiting manager know. \n",
      "Examples of adjustments include providing \n",
      "documents in large print or braille, allowing \n",
      "more time for a test or interview or providing \n",
      "assistance at an assessment centre. Why we assess behaviours \n",
      "How we assess behaviours\n",
      "1: It is important to remember that recruiting managers will choose a selection of \n",
      "behaviours which are best suited to the specific job role. You will not be asked to \n",
      "demonstrate all Civil Service Behaviours for one role.\n",
      "The examples of the behaviours are designed to give an overview of what is \n",
      "expected of individuals at each level. There is no expectation that all individuals will \n",
      "need to demonstrate every part of each example to be successful.\n",
      "1: well and that motivate us.\n",
      "• Ability  - the aptitude or potential to perform \n",
      "to the required standard.\n",
      "• Experience  - the knowledge or mastery \n",
      "of an activity or subject gained through \n",
      "involvement in or exposure to it.  \n",
      "• Technical  - the demonstration of \n",
      "specific professional skills, knowledge or \n",
      "qualifications.\n",
      "Not all elements are relevant to every role, so \n",
      "the makeup of the Success Profile should be \n",
      "different for different types of job to improve the \n",
      "chances of getting the best person for the role. \n",
      "Civil Service Behaviours  \n",
      "Behaviours are the actions and activities that people do which  \n",
      "result in effective performance in a job.  \n",
      "The Civil Service has defined a set of behaviours that, when demonstrated, are \n",
      "associated with job success. Civil Service Behaviours are specific to the grade \n",
      "level of the job role. \n",
      "It is important to remember that recruiting managers will choose a selection of\n"
     ]
    }
   ],
   "source": [
    "docs = faiss_index.similarity_search(\"What places can I use to find job opportunities\", k=4)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:22:48.743353Z",
     "iopub.status.busy": "2024-03-23T15:22:48.742854Z",
     "iopub.status.idle": "2024-03-23T15:22:48.816566Z",
     "shell.execute_reply": "2024-03-23T15:22:48.816060Z",
     "shell.execute_reply.started": "2024-03-23T15:22:48.743353Z"
    },
    "id": "pruAucjf84ri"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation', model=model, tokenizer=tokenizer, max_new_tokens=512, temperature=0, do_sample=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:22:48.946471Z",
     "iopub.status.busy": "2024-03-23T15:22:48.946471Z",
     "iopub.status.idle": "2024-03-23T15:22:49.799251Z",
     "shell.execute_reply": "2024-03-23T15:22:49.798745Z",
     "shell.execute_reply.started": "2024-03-23T15:22:48.946471Z"
    },
    "id": "blXXUip-8m36"
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=faiss_index.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2024-03-23T15:22:49.800750Z",
     "iopub.status.busy": "2024-03-23T15:22:49.800253Z",
     "iopub.status.idle": "2024-03-23T15:22:49.834350Z",
     "shell.execute_reply": "2024-03-23T15:22:49.833844Z",
     "shell.execute_reply.started": "2024-03-23T15:22:49.800750Z"
    },
    "id": "X5m25D3G_KkA",
    "outputId": "1566a933-36c7-43c8-920b-b1e1353c6b4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nWrite a hello world program<end_of_turn>\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template([{ \"role\": \"user\", \"content\": \"Write a hello world program\" }], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:29:10.525682Z",
     "iopub.status.busy": "2024-03-23T15:29:10.525178Z",
     "iopub.status.idle": "2024-03-23T15:29:10.529572Z",
     "shell.execute_reply": "2024-03-23T15:29:10.529066Z",
     "shell.execute_reply.started": "2024-03-23T15:29:10.525682Z"
    },
    "id": "nSaiyGvo_lPX"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"<bos><start_of_turn>user\\n\"\n",
    "    \"Using either your own expert knowledge (no tag), enriched with the following contextual data (denoted by CONTEXT), \"\n",
    "    \"please answer my prompt (denoted by PROMPT)\\nIf there you cannot answer from the context alone, please let \"\n",
    "    \"the user know, but still answer with your expert knowledge.\\n\"\n",
    "    \"CONTEXT\\n{context}\\nPROMPT\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:29:11.415317Z",
     "iopub.status.busy": "2024-03-23T15:29:11.415317Z",
     "iopub.status.idle": "2024-03-23T15:29:11.548716Z",
     "shell.execute_reply": "2024-03-23T15:29:11.548213Z",
     "shell.execute_reply.started": "2024-03-23T15:29:11.415317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='PROFILE  Tin dicatas con Ita, caus ex mil us ceresil larisque crunimpliam\\nACCOUNT EXECUTIVE KANDA CONSULTING   / JUNE 2021 – PRESENT • Produced political research and reports to support the redevelopment schemes of a number of high-profile clients in the Central London development industry and local authorities.  • Gathered feedback on a wide range of regeneration projects in Westminster and The City of London and produced detailed reports including quantitative and qualitative analysis on this feedback to the client.  • Social Media and website content management – delivered high quality content for bespoke consultation websites and newsletters as well as the Company’s LinkedIn feed. • Stakeholder engagement – communicated and maintained positive relationships with a wide range of community, political and commercial stakeholders.  • Delivered research to support Directors in securing new business.', metadata={'source': 'C:\\\\Users\\\\chris\\\\repos\\\\ml\\\\notebooks\\\\data\\\\Sophia+CV+2021.pdf', 'page': 0}),\n",
       " Document(page_content='• English - Native • Greek - Native • Spanish - C1 • Brazilian Portuguese – B2 • Mandarin Chinese - Basic MANAGEMENT INTERN/LEAD VOLUNTEER OXFAM GB/ 2018 – PRESENT • Supervised and trained a team of between 10 to 15 volunteers, delegating responsibilities and ensuring volunteer well-being. • Processed donations and provided excellent customer service, encouraging donors to sign up for GiftAid and walking them through the process. • Compiled daily takings reports as well as overseeing daily cash register operations and cash management. • Managed the store social media accounts through Hootsuite.', metadata={'source': 'C:\\\\Users\\\\chris\\\\repos\\\\ml\\\\notebooks\\\\data\\\\Sophia+CV+2021.pdf', 'page': 0}),\n",
       " Document(page_content='Level 2 – EO or equivalent|  4\\nCivil Service Behaviours - Level 2Examples of behaviours at this level are:\\nWorking Together\\nDevelop a range of contacts outside own team and identify opportunities to share \\nknowledge, information and learning. Show genuine interest when listening to others. \\nContribute to an inclusive working environment where all opinions and challenges \\nare listened to and all individual needs are taken into account. Ensure it is clear that \\nbullying, harassment and discrimination are unacceptable. Offer support and help to \\ncolleagues when in need, including consideration of your own and their wellbeing. \\nChange ways of working to aid cooperation within and between teams in order to \\nachieve results.  \\nDeveloping Self and Others\\nIdentify gaps in own and team’s skills and knowledge. Set and consistently meet \\ndevelopment objectives. Seek learning opportunities. Support the development plans', metadata={'source': 'C:\\\\Users\\\\chris\\\\repos\\\\ml\\\\notebooks\\\\data\\\\CS_Behaviours_2018.pdf', 'page': 8}),\n",
       " Document(page_content='Level 2 – EO or equivalent|  3\\nCivil Service Behaviours - Level 2Examples of behaviours at this level are:\\nSeeing the Big Picture\\nUnderstand how your work and the work of your team supports wider objectives and \\nmeets the diverse needs of stakeholders. Keep up to date with the issues that affect \\nyour work area. Take a keen interest in expanding knowledge in areas related to your \\nwork. Focus on overall goals and not just specific tasks to meet priorities.\\nChanging and Improving\\nRegularly review own and team’s work and take the initiative to suggest ideas to make \\nimprovements. Give feedback on changes in a constructive manner. Take a positive, \\nopen approach to the possibility of change and encourage others to do the same. Help \\nothers to understand changes and the reasons they are being put in place. Identify and \\nact on the effects changes are having on your role and that of the team. Look for ways', metadata={'source': 'C:\\\\Users\\\\chris\\\\repos\\\\ml\\\\notebooks\\\\data\\\\CS_Behaviours_2018.pdf', 'page': 7})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_index.similarity_search('what is the key contribution of llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "execution": {
     "iopub.execute_input": "2024-03-23T15:38:08.826742Z",
     "iopub.status.busy": "2024-03-23T15:38:08.826241Z",
     "iopub.status.idle": "2024-03-23T15:38:08.830650Z",
     "shell.execute_reply": "2024-03-23T15:38:08.830650Z",
     "shell.execute_reply.started": "2024-03-23T15:38:08.826742Z"
    },
    "id": "nXx2dmHMBNvx",
    "outputId": "a8394832-4483-4deb-bf70-53340a1152f9"
   },
   "outputs": [],
   "source": [
    "# Using the newer chains api\n",
    "from langchain_core.runnables import RunnableGenerator\n",
    "from typing import Iterable\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "\n",
    "def parse(message):\n",
    "    return message.split(\"<start_of_turn>model\\n\")\n",
    "\n",
    "def ret_func(query_input):\n",
    "    docs = faiss_index.max_marginal_relevance_search(query_input['prompt'], k=7, fetch_k=20, lambda_mult=0.4)\n",
    "    # docs = retriever_from_llm.invoke(query_input['prompt'])\n",
    "\n",
    "    print('\\n\\n')\n",
    "    doc_name = doc.metadata[\"source\"].split(\"\\\\\")[-1]\n",
    "    contexts = [f'Source: {doc_name} - Data: {doc.metadata[\"page\"]}: {doc.page_content}' for doc in docs]\n",
    "    return '\\n'.join(contexts)\n",
    "\n",
    "# print(ret_func({'prompt':'what are the key contributions of llama2'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:39:42.641225Z",
     "iopub.status.busy": "2024-03-23T15:39:42.641225Z",
     "iopub.status.idle": "2024-03-23T15:40:00.675832Z",
     "shell.execute_reply": "2024-03-23T15:40:00.674826Z",
     "shell.execute_reply.started": "2024-03-23T15:39:42.641225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\miniconda3\\envs\\ai\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    {\n",
    "        # \"context\": retriever_from_llm,\n",
    "        \"context\": ret_func,\n",
    "        \"prompt\": itemgetter('prompt'),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parse\n",
    "\n",
    ")\n",
    "\n",
    "output = chain.invoke({\"prompt\": \"Summarise my work experience from my CV and give a list of potential future opportunities with explanations of why they are suitable and industries they can be applied in\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:40:00.677334Z",
     "iopub.status.busy": "2024-03-23T15:40:00.676834Z",
     "iopub.status.idle": "2024-03-23T15:40:00.680631Z",
     "shell.execute_reply": "2024-03-23T15:40:00.680124Z",
     "shell.execute_reply.started": "2024-03-23T15:40:00.677334Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T15:40:00.682131Z",
     "iopub.status.busy": "2024-03-23T15:40:00.681632Z",
     "iopub.status.idle": "2024-03-23T15:40:00.685654Z",
     "shell.execute_reply": "2024-03-23T15:40:00.685147Z",
     "shell.execute_reply.started": "2024-03-23T15:40:00.682131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Summary of Work Experience\n",
      "\n",
      "Based on the context, I have gained experience in various roles, demonstrating my ability to adapt to different situations and contribute to achieving organizational goals. My key responsibilities include:\n",
      "\n",
      "* **Project Management:** Planning, organizing, and executing projects to deliver successful outcomes.\n",
      "* **Communication:** Engaging in clear and concise communication with stakeholders, both verbally and in writing.\n",
      "* **Teamwork:** Collaborating effectively with colleagues from diverse backgrounds and fostering a positive team environment.\n",
      "* **Problem-solving:** Identifying and resolving challenges through creative and innovative solutions.\n",
      "* **Strategic Thinking:** Developing and implementing strategic plans that align with organizational goals.\n",
      "* **Leadership:** Inspiring and motivating team members to achieve their full potential.\n",
      "\n",
      "## Potential Future Opportunities\n",
      "\n",
      "Based on my skills and experience, I am suitable for the following potential future opportunities:\n",
      "\n",
      "* **Project Manager:** Leading and managing complex projects in various industries, such as construction, technology, and healthcare.\n",
      "* **Communications Specialist:** Developing and implementing effective communication strategies that effectively engage stakeholders.\n",
      "* **Change Management Specialist:** Leading and facilitating successful organizational change initiatives.\n",
      "* **Strategic Planner:** Developing and implementing strategic plans that ensure long-term success.\n",
      "* **Executive Leadership:** Providing strategic guidance and leadership to teams and organizations.\n",
      "\n",
      "These opportunities align with my strengths and interests, allowing me to utilize my skills and contribute to achieving positive outcomes in various settings.\n"
     ]
    }
   ],
   "source": [
    "print(output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T23:16:13.151728Z",
     "iopub.status.busy": "2024-03-22T23:16:13.151223Z",
     "iopub.status.idle": "2024-03-22T23:16:24.077851Z",
     "shell.execute_reply": "2024-03-22T23:16:24.077345Z",
     "shell.execute_reply.started": "2024-03-22T23:16:13.151728Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\miniconda3\\envs\\ai\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are approaches to deal with problems where the number of features is greater than the number of samples:\n",
      "\n",
      "- **Resampling methods:** Resampling methods involve repeatedly drawing samples from the training set and refitting the model on each sample. This approach can help to reduce the overfitting of the model.\n",
      "- **Tree pruning:** Tree pruning is a technique that can be used to reduce the complexity of a tree-based model. This approach can help to improve the performance of the model on the test set.\n",
      "- **Feature selection:** Feature selection is a technique that can be used to identify the most important features in a dataset. This approach can help to improve the performance of the model on the test set.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'prompt': 'What are approaches to deal with problems where the number of features is greater than the number of samples?'})[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T23:19:37.458236Z",
     "iopub.status.busy": "2024-03-22T23:19:37.457737Z",
     "iopub.status.idle": "2024-03-22T23:19:37.472206Z",
     "shell.execute_reply": "2024-03-22T23:19:37.471699Z",
     "shell.execute_reply.started": "2024-03-22T23:19:37.458236Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MultiQueryRetriever._get_relevant_documents() missing 1 required keyword-only argument: 'run_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m retriever_from_llm\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat are approaches to deal with problems where the number of features is greater than the number of samples?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: MultiQueryRetriever._get_relevant_documents() missing 1 required keyword-only argument: 'run_manager'"
     ]
    }
   ],
   "source": [
    "retriever_from_llm._get_relevant_documents('What are approaches to deal with problems where the number of features is greater than the number of samples?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T23:18:40.827386Z",
     "iopub.status.busy": "2024-03-22T23:18:40.826887Z",
     "iopub.status.idle": "2024-03-22T23:18:40.833204Z",
     "shell.execute_reply": "2024-03-22T23:18:40.832697Z",
     "shell.execute_reply.started": "2024-03-22T23:18:40.827386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class MultiQueryRetriever(BaseRetriever):\n",
      "    \"\"\"Given a query, use an LLM to write a set of queries.\n",
      "\n",
      "    Retrieve docs for each query. Return the unique union of all retrieved docs.\n",
      "    \"\"\"\n",
      "\n",
      "    retriever: BaseRetriever\n",
      "    llm_chain: LLMChain\n",
      "    verbose: bool = True\n",
      "    parser_key: str = \"lines\"\n",
      "    \"\"\"DEPRECATED. parser_key is no longer used and should not be specified.\"\"\"\n",
      "    include_original: bool = False\n",
      "    \"\"\"Whether to include the original query in the list of generated queries.\"\"\"\n",
      "\n",
      "    @classmethod\n",
      "    def from_llm(\n",
      "        cls,\n",
      "        retriever: BaseRetriever,\n",
      "        llm: BaseLanguageModel,\n",
      "        prompt: PromptTemplate = DEFAULT_QUERY_PROMPT,\n",
      "        parser_key: Optional[str] = None,\n",
      "        include_original: bool = False,\n",
      "    ) -> \"MultiQueryRetriever\":\n",
      "        \"\"\"Initialize from llm using default template.\n",
      "\n",
      "        Args:\n",
      "            retriever: retriever to query documents from\n",
      "            llm: llm for query generation using DEFAULT_QUERY_PROMPT\n",
      "            include_original: Whether to include the original query in the list of\n",
      "                generated queries.\n",
      "\n",
      "        Returns:\n",
      "            MultiQueryRetriever\n",
      "        \"\"\"\n",
      "        output_parser = LineListOutputParser()\n",
      "        llm_chain = LLMChain(llm=llm, prompt=prompt, output_parser=output_parser)\n",
      "        return cls(\n",
      "            retriever=retriever,\n",
      "            llm_chain=llm_chain,\n",
      "            include_original=include_original,\n",
      "        )\n",
      "\n",
      "    async def _aget_relevant_documents(\n",
      "        self,\n",
      "        query: str,\n",
      "        *,\n",
      "        run_manager: AsyncCallbackManagerForRetrieverRun,\n",
      "    ) -> List[Document]:\n",
      "        \"\"\"Get relevant documents given a user query.\n",
      "\n",
      "        Args:\n",
      "            question: user query\n",
      "\n",
      "        Returns:\n",
      "            Unique union of relevant documents from all generated queries\n",
      "        \"\"\"\n",
      "        queries = await self.agenerate_queries(query, run_manager)\n",
      "        if self.include_original:\n",
      "            queries.append(query)\n",
      "        documents = await self.aretrieve_documents(queries, run_manager)\n",
      "        return self.unique_union(documents)\n",
      "\n",
      "    async def agenerate_queries(\n",
      "        self, question: str, run_manager: AsyncCallbackManagerForRetrieverRun\n",
      "    ) -> List[str]:\n",
      "        \"\"\"Generate queries based upon user input.\n",
      "\n",
      "        Args:\n",
      "            question: user query\n",
      "\n",
      "        Returns:\n",
      "            List of LLM generated queries that are similar to the user input\n",
      "        \"\"\"\n",
      "        response = await self.llm_chain.acall(\n",
      "            inputs={\"question\": question}, callbacks=run_manager.get_child()\n",
      "        )\n",
      "        lines = response[\"text\"]\n",
      "        if self.verbose:\n",
      "            logger.info(f\"Generated queries: {lines}\")\n",
      "        return lines\n",
      "\n",
      "    async def aretrieve_documents(\n",
      "        self, queries: List[str], run_manager: AsyncCallbackManagerForRetrieverRun\n",
      "    ) -> List[Document]:\n",
      "        \"\"\"Run all LLM generated queries.\n",
      "\n",
      "        Args:\n",
      "            queries: query list\n",
      "\n",
      "        Returns:\n",
      "            List of retrieved Documents\n",
      "        \"\"\"\n",
      "        document_lists = await asyncio.gather(\n",
      "            *(\n",
      "                self.retriever.aget_relevant_documents(\n",
      "                    query, callbacks=run_manager.get_child()\n",
      "                )\n",
      "                for query in queries\n",
      "            )\n",
      "        )\n",
      "        return [doc for docs in document_lists for doc in docs]\n",
      "\n",
      "    def _get_relevant_documents(\n",
      "        self,\n",
      "        query: str,\n",
      "        *,\n",
      "        run_manager: CallbackManagerForRetrieverRun,\n",
      "    ) -> List[Document]:\n",
      "        \"\"\"Get relevant documents given a user query.\n",
      "\n",
      "        Args:\n",
      "            question: user query\n",
      "\n",
      "        Returns:\n",
      "            Unique union of relevant documents from all generated queries\n",
      "        \"\"\"\n",
      "        queries = self.generate_queries(query, run_manager)\n",
      "        if self.include_original:\n",
      "            queries.append(query)\n",
      "        documents = self.retrieve_documents(queries, run_manager)\n",
      "        return self.unique_union(documents)\n",
      "\n",
      "    def generate_queries(\n",
      "        self, question: str, run_manager: CallbackManagerForRetrieverRun\n",
      "    ) -> List[str]:\n",
      "        \"\"\"Generate queries based upon user input.\n",
      "\n",
      "        Args:\n",
      "            question: user query\n",
      "\n",
      "        Returns:\n",
      "            List of LLM generated queries that are similar to the user input\n",
      "        \"\"\"\n",
      "        response = self.llm_chain(\n",
      "            {\"question\": question}, callbacks=run_manager.get_child()\n",
      "        )\n",
      "        lines = response[\"text\"]\n",
      "        if self.verbose:\n",
      "            logger.info(f\"Generated queries: {lines}\")\n",
      "        return lines\n",
      "\n",
      "    def retrieve_documents(\n",
      "        self, queries: List[str], run_manager: CallbackManagerForRetrieverRun\n",
      "    ) -> List[Document]:\n",
      "        \"\"\"Run all LLM generated queries.\n",
      "\n",
      "        Args:\n",
      "            queries: query list\n",
      "\n",
      "        Returns:\n",
      "            List of retrieved Documents\n",
      "        \"\"\"\n",
      "        documents = []\n",
      "        for query in queries:\n",
      "            docs = self.retriever.get_relevant_documents(\n",
      "                query, callbacks=run_manager.get_child()\n",
      "            )\n",
      "            documents.extend(docs)\n",
      "        return documents\n",
      "\n",
      "    def unique_union(self, documents: List[Document]) -> List[Document]:\n",
      "        \"\"\"Get unique Documents.\n",
      "\n",
      "        Args:\n",
      "            documents: List of retrieved Documents\n",
      "\n",
      "        Returns:\n",
      "            List of unique retrieved Documents\n",
      "        \"\"\"\n",
      "        return _unique_documents(documents)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(MultiQueryRetriever))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2c1780a44ba24430b646cf498359fedd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4d7eccb740a4fa5a972cd56966e4ca3",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_53c2815db3554e369d7f7892a05459c4",
      "value": 2
     }
    },
    "44a85744e1a04120927f20b9091c2157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "485ca683a09043ab991da114ff9c299b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aac14b2d2f6e4e7ca6f290ad3b443ea5",
      "placeholder": "​",
      "style": "IPY_MODEL_ffbfc9148193426a9eb7c6bd29ceea19",
      "value": " 2/2 [00:20&lt;00:00,  8.34s/it]"
     }
    },
    "53c2815db3554e369d7f7892a05459c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aac14b2d2f6e4e7ca6f290ad3b443ea5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b54b9ab58688455d83c3ea53842f5a93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbf66deef35b4a969d616d1094c84f39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6748edf2a414b9f9aba069de012406f",
      "placeholder": "​",
      "style": "IPY_MODEL_44a85744e1a04120927f20b9091c2157",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "e225606e93e249a981530828d2ba9727": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbf66deef35b4a969d616d1094c84f39",
       "IPY_MODEL_2c1780a44ba24430b646cf498359fedd",
       "IPY_MODEL_485ca683a09043ab991da114ff9c299b"
      ],
      "layout": "IPY_MODEL_b54b9ab58688455d83c3ea53842f5a93"
     }
    },
    "f4d7eccb740a4fa5a972cd56966e4ca3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6748edf2a414b9f9aba069de012406f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffbfc9148193426a9eb7c6bd29ceea19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
