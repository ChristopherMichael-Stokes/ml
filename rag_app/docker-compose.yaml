services:
  llama_backend: 
    image: llama-cpp-python/llm_backend
    build:
      context: .
      dockerfile: ./Dockerfile
    volumes:
      - ./llama3:/app/llama3
      - ./bge-large:/app/bge-large
      - ./output:/app/output
    ports: 
      - 7860:7860
      - 8000:8000
    # Pytorch / cuda configuration options
    ipc: host
    ulimits:
      nofile:
        soft: 65535
        hard: 65535
      memlock:
        soft: -1
        hard: -1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
